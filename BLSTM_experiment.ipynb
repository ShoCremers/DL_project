{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prerequisite-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dependencies (as taken from assignment 6)\n",
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from joblib import load\n",
    "import graycode\n",
    "\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "#Additional Setup to use Tensorboard\n",
    "!pip install -q tensorflow\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-dining",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "banner-display",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day-ahead Price [EUR/MWh]</th>\n",
       "      <th>time</th>\n",
       "      <th>tempC</th>\n",
       "      <th>windspeedKmph</th>\n",
       "      <th>winddirDegree</th>\n",
       "      <th>precipMM</th>\n",
       "      <th>humidity</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-05 00:00:00</th>\n",
       "      <td>-0.243620</td>\n",
       "      <td>2400</td>\n",
       "      <td>-1.722376</td>\n",
       "      <td>-0.422598</td>\n",
       "      <td>0.563889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.163731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05 01:00:00</th>\n",
       "      <td>-0.316395</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.722376</td>\n",
       "      <td>-0.297098</td>\n",
       "      <td>0.572222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.058525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05 02:00:00</th>\n",
       "      <td>-0.439933</td>\n",
       "      <td>200</td>\n",
       "      <td>-1.722376</td>\n",
       "      <td>-0.171598</td>\n",
       "      <td>0.580556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>2.058525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05 03:00:00</th>\n",
       "      <td>-0.625914</td>\n",
       "      <td>300</td>\n",
       "      <td>-1.722376</td>\n",
       "      <td>-0.046098</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.953318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05 04:00:00</th>\n",
       "      <td>-0.626363</td>\n",
       "      <td>400</td>\n",
       "      <td>-1.722376</td>\n",
       "      <td>-0.171598</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.953318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 19:00:00</th>\n",
       "      <td>0.877205</td>\n",
       "      <td>1900</td>\n",
       "      <td>-1.130468</td>\n",
       "      <td>-0.673599</td>\n",
       "      <td>0.602778</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-1.728907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 20:00:00</th>\n",
       "      <td>0.665169</td>\n",
       "      <td>2000</td>\n",
       "      <td>-1.130468</td>\n",
       "      <td>-0.422598</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-1.728907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 21:00:00</th>\n",
       "      <td>0.469755</td>\n",
       "      <td>2100</td>\n",
       "      <td>-1.130468</td>\n",
       "      <td>-0.171598</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-1.623701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 22:00:00</th>\n",
       "      <td>0.443699</td>\n",
       "      <td>2200</td>\n",
       "      <td>-1.278445</td>\n",
       "      <td>-0.422598</td>\n",
       "      <td>0.747222</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-1.413288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 23:00:00</th>\n",
       "      <td>0.461668</td>\n",
       "      <td>2300</td>\n",
       "      <td>-1.426422</td>\n",
       "      <td>-0.673599</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.91</td>\n",
       "      <td>-1.308081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52512 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Day-ahead Price [EUR/MWh]  time     tempC  windspeedKmph  \\\n",
       "datetime                                                                        \n",
       "2015-01-05 00:00:00                  -0.243620  2400 -1.722376      -0.422598   \n",
       "2015-01-05 01:00:00                  -0.316395   100 -1.722376      -0.297098   \n",
       "2015-01-05 02:00:00                  -0.439933   200 -1.722376      -0.171598   \n",
       "2015-01-05 03:00:00                  -0.625914   300 -1.722376      -0.046098   \n",
       "2015-01-05 04:00:00                  -0.626363   400 -1.722376      -0.171598   \n",
       "...                                        ...   ...       ...            ...   \n",
       "2020-12-31 19:00:00                   0.877205  1900 -1.130468      -0.673599   \n",
       "2020-12-31 20:00:00                   0.665169  2000 -1.130468      -0.422598   \n",
       "2020-12-31 21:00:00                   0.469755  2100 -1.130468      -0.171598   \n",
       "2020-12-31 22:00:00                   0.443699  2200 -1.278445      -0.422598   \n",
       "2020-12-31 23:00:00                   0.461668  2300 -1.426422      -0.673599   \n",
       "\n",
       "                     winddirDegree  precipMM  humidity  pressure  \n",
       "datetime                                                          \n",
       "2015-01-05 00:00:00       0.563889  0.000000      0.96  2.163731  \n",
       "2015-01-05 01:00:00       0.572222  0.000000      0.96  2.058525  \n",
       "2015-01-05 02:00:00       0.580556  0.000000      0.96  2.058525  \n",
       "2015-01-05 03:00:00       0.588889  0.000000      0.96  1.953318  \n",
       "2015-01-05 04:00:00       0.575000  0.000000      0.96  1.953318  \n",
       "...                            ...       ...       ...       ...  \n",
       "2020-12-31 19:00:00       0.602778  0.054545      0.94 -1.728907  \n",
       "2020-12-31 20:00:00       0.625000  0.036364      0.93 -1.728907  \n",
       "2020-12-31 21:00:00       0.650000  0.054545      0.93 -1.623701  \n",
       "2020-12-31 22:00:00       0.747222  0.036364      0.92 -1.413288  \n",
       "2020-12-31 23:00:00       0.847222  0.018182      0.91 -1.308081  \n",
       "\n",
       "[52512 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('day_ahead.csv')\n",
    "df = df.set_index('datetime')\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "included-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "dayahead_scaler = load('dayahead_scaler.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-inspection",
   "metadata": {},
   "source": [
    "# Time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "historical-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = (df['time'].values/100).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recreational-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incremental representation of time\n",
    "time_increment = time/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "banner-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gray code binary\n",
    "time_gray_code = np.empty([len(time), 5])\n",
    "for i in range(len(time)):\n",
    "    gray_code_str = '{:05b}'.format(graycode.tc_to_gray_code(time[i]))\n",
    "    time_gray_code[i] = np.array(list(gray_code_str)).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "floppy-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutually exclusive binary representation\n",
    "time_exclusive = np.zeros([len(time), 24])\n",
    "for i in range(len(time)):\n",
    "    time_exclusive[i][time[i] - 1] = 1\n",
    "time_exclusive = time_exclusive[:,::-1] # reverse array to correspond to binary representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-values",
   "metadata": {},
   "source": [
    "# Dataset with time variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coupled-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no time variable\n",
    "df0 = df.drop(columns=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "assigned-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copies of original dataset\n",
    "df1 = df0.copy()\n",
    "df2 = df0.copy()\n",
    "df3 = df0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "backed-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incremental time representation\n",
    "df1['time_increment'] = time_increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "commercial-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gray code time representation\n",
    "time_gray_code.shape\n",
    "for i in range(time_gray_code.shape[1]):\n",
    "    df2['gc_'+str(i)] = time_gray_code[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "widespread-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutually exclusive time representation\n",
    "for i in range(time_exclusive.shape[1]):\n",
    "    df3['ex_'+str(i)] = time_exclusive[:,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-beast",
   "metadata": {},
   "source": [
    "# Create Torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "outstanding-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_torch_dataset(df, seq_length):\n",
    "    \n",
    "    delta = pd.Timedelta(seq_length, unit ='h')\n",
    "    # define 1 hour object for convenience when using datetime as index in the dataframe to not include the last item\n",
    "    hours_12 = pd.Timedelta(12, unit ='h') # used mostly for empty 12 hours \n",
    "    hour = pd.Timedelta(1, unit ='h')\n",
    "    day = pd.Timedelta(1, unit ='d')\n",
    "    \n",
    "    ### creating training dataset\n",
    "    train_y_start = dt.datetime(2015, 1, 5, 0, 0) + (delta+hours_12).ceil('1d')\n",
    "    train_end = dt.datetime(2020, 10, 31, 23, 0)\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    while train_y_start + day - hour <= train_end:\n",
    "        train_x_start = train_y_start - delta - hours_12\n",
    "\n",
    "\n",
    "        #print(train_x_start, train_y_start)\n",
    "        train_x.append(df[train_x_start:train_x_start+delta - hour].values)\n",
    "        train_y.append(df[train_y_start:train_y_start+day - hour]['Day-ahead Price [EUR/MWh]'].values)\n",
    "\n",
    "        train_y_start += day\n",
    "\n",
    "    train_x = np.asarray(train_x)\n",
    "    train_y = np.asarray(train_y)\n",
    "    \n",
    "    \n",
    "    ### creating validation dataset\n",
    "    val_y_start = dt.datetime(2020, 11, 1, 0, 0)\n",
    "    val_end = dt.datetime(2020, 11, 30, 23, 0)\n",
    "\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    while val_y_start + day - hour <= val_end:\n",
    "        val_x_start = val_y_start - delta - hours_12\n",
    "\n",
    "        val_x.append(df[val_x_start:val_x_start+delta - hour].values)\n",
    "        val_y.append(df[val_y_start:val_y_start+day - hour]['Day-ahead Price [EUR/MWh]'].values)\n",
    "\n",
    "        val_y_start += day\n",
    "\n",
    "    val_x = np.asarray(val_x)\n",
    "    val_y = np.asarray(val_y)\n",
    "    \n",
    "    ### creating testing dataset\n",
    "    test_y_start = dt.datetime(2020, 12, 1, 0, 0)\n",
    "    test_end = dt.datetime(2020, 12, 31, 23, 0)\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    while test_y_start + day - hour <= test_end:\n",
    "        test_x_start = test_y_start - delta - hours_12\n",
    "\n",
    "        test_x.append(df[test_x_start:test_x_start+delta - hour].values)\n",
    "        test_y.append(df[test_y_start:test_y_start+day - hour]['Day-ahead Price [EUR/MWh]'].values)\n",
    "\n",
    "        test_y_start += day\n",
    "\n",
    "    test_x = np.asarray(test_x)\n",
    "    test_y = np.asarray(test_y)\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-blame",
   "metadata": {},
   "source": [
    "# Define BLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "magnetic-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, quantiles):\n",
    "        super(BLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.fc = nn.Linear(hidden_dim*2, output_dim) # multiply hidden_dim by 2 because bidirectional\n",
    "        \n",
    "        self.quantiles = quantiles\n",
    "        self.num_quantiles = len(quantiles)\n",
    "        self.out_shape = len(quantiles)\n",
    "        \n",
    "        final_layers = [\n",
    "            nn.Linear(hidden_dim*2, output_dim) for _ in range(len(self.quantiles))\n",
    "        ]\n",
    "        self.final_layers = nn.ModuleList(final_layers)\n",
    "        \n",
    "    def add_noise_to_weights(self):\n",
    "        with torch.no_grad():\n",
    "            # add noise to lstm weights\n",
    "            for weights in model.lstm._all_weights:\n",
    "                for weight in weights:\n",
    "                    noise = torch.normal(0, 0.01, size=self.lstm._parameters[weight].size())\n",
    "                    self.lstm._parameters[weight].add_(noise)\n",
    "            # add noise to linear layer weights, most likely unnecessary\n",
    "#             for layer in self.final_layers:\n",
    "#                 if hasattr(layer, 'weight'):\n",
    "#                     noise = torch.normal(0, 0.05, size=layer.weight.size())\n",
    "#                     layer.weight.add_(noise)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).requires_grad_() #hidden layer output\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_dim).requires_grad_() \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        # Index hidden state of last time step\n",
    "#         _out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return torch.stack([layer(out[:, -1, :]) for layer in self.final_layers], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-discount",
   "metadata": {},
   "source": [
    "# (Hyper)parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sealed-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters to set\n",
    "seq_lengths = [12, 24, 36, 48, 72]\n",
    "dfs = [df0, df1, df2, df3]\n",
    "hidden_dim_ = [4, 8, 16, 32, 64, 128] # no. of neurons in hidden layer\n",
    "num_layers_ = [1, 2, 3, 4] # no of hidden layers \n",
    "\n",
    "\n",
    "# predetermined parameters\n",
    "output_dim = 24 \n",
    "num_epochs = 500\n",
    "batch_size = 64\n",
    "quantiles = [.01,0.05, 0.10,0.25, .5, 0.75, 0.90, 0.95, .99]\n",
    "patience = 10 # for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-ethiopia",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-monaco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  1  hidden dim:  4\n",
      "Epoch  10 training loss:  0.9755202338976019 validation loss:  0.8133288621902466\n",
      "Early stopping: Epoch  15 training loss:  0.9565267036942875 validation loss:  0.8042422533035278\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  2  hidden dim:  4\n",
      "Epoch  10 training loss:  1.0075595343814177 validation loss:  0.8146846294403076\n",
      "Early stopping: Epoch  18 training loss:  0.9645799678914687 validation loss:  0.8119679093360901\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  3  hidden dim:  4\n",
      "Epoch  10 training loss:  1.0506703117314506 validation loss:  0.8032760620117188\n",
      "Early stopping: Epoch  20 training loss:  0.9324082048500285 validation loss:  0.8292312622070312\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  4  hidden dim:  4\n",
      "Epoch  10 training loss:  1.2836347923559301 validation loss:  0.9158518314361572\n",
      "Epoch  20 training loss:  0.9620685121592354 validation loss:  0.8387176394462585\n",
      "Epoch  30 training loss:  0.9276277422904968 validation loss:  0.8161103129386902\n",
      "Early stopping: Epoch  32 training loss:  0.9124054961344775 validation loss:  0.8163341283798218\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  1  hidden dim:  8\n",
      "Epoch  10 training loss:  0.9532567771042094 validation loss:  0.8064015507698059\n",
      "Epoch  20 training loss:  0.9253534411682802 validation loss:  0.7916511297225952\n",
      "Epoch  30 training loss:  0.8974046146168428 validation loss:  0.7784637212753296\n",
      "Early stopping: Epoch  38 training loss:  0.8879114073865554 validation loss:  0.7888235449790955\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  2  hidden dim:  8\n",
      "Epoch  10 training loss:  0.9873359851977405 validation loss:  0.8185994625091553\n",
      "Epoch  20 training loss:  0.9476705754504484 validation loss:  0.7901946306228638\n",
      "Epoch  30 training loss:  0.9115443948437186 validation loss:  0.7869293093681335\n",
      "Epoch  40 training loss:  0.8869796763448154 validation loss:  0.7535902857780457\n",
      "Epoch  50 training loss:  0.8651691149262821 validation loss:  0.7385357618331909\n",
      "Epoch  60 training loss:  0.8525932431221008 validation loss:  0.7714850306510925\n",
      "Early stopping: Epoch  62 training loss:  0.8467841446399689 validation loss:  0.785359263420105\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  3  hidden dim:  8\n",
      "Epoch  10 training loss:  0.9923915109213661 validation loss:  0.8056676387786865\n",
      "Epoch  20 training loss:  0.9330279178479138 validation loss:  0.7944808006286621\n",
      "Epoch  30 training loss:  0.9284268179360557 validation loss:  0.7796411514282227\n",
      "Early stopping: Epoch  38 training loss:  0.8800828439347884 validation loss:  0.8174804449081421\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  4  hidden dim:  8\n",
      "Epoch  10 training loss:  1.1301280382801504 validation loss:  0.8610610365867615\n",
      "Epoch  20 training loss:  0.9457068040090448 validation loss:  0.8407462239265442\n",
      "Early stopping: Epoch  25 training loss:  0.9194840999210582 validation loss:  0.8235429525375366\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  1  hidden dim:  16\n",
      "Epoch  10 training loss:  0.9597756231532377 validation loss:  0.7989414930343628\n",
      "Epoch  20 training loss:  0.9339858030571657 validation loss:  0.7811594009399414\n",
      "Epoch  30 training loss:  0.9204816432560191 validation loss:  0.7743167281150818\n",
      "Epoch  40 training loss:  0.8780514930977541 validation loss:  0.7870066165924072\n",
      "Early stopping: Epoch  48 training loss:  0.8623863598879646 validation loss:  0.77244633436203\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  2  hidden dim:  16\n",
      "Epoch  10 training loss:  0.9833646153702456 validation loss:  0.8170008063316345\n",
      "Early stopping: Epoch  14 training loss:  0.9486108825487249 validation loss:  0.7951017022132874\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  3  hidden dim:  16\n",
      "Epoch  10 training loss:  1.0225535052664139 validation loss:  0.8356679081916809\n",
      "Epoch  20 training loss:  0.9500768307377311 validation loss:  0.7960259318351746\n",
      "Epoch  30 training loss:  0.9156646360369289 validation loss:  0.7816893458366394\n",
      "Epoch  40 training loss:  0.8621321162756752 validation loss:  0.7437338829040527\n",
      "Epoch  50 training loss:  0.8091631759615505 validation loss:  0.79004967212677\n",
      "Early stopping: Epoch  53 training loss:  0.790153707651531 validation loss:  0.8564589023590088\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  4  hidden dim:  16\n",
      "Epoch  10 training loss:  0.9962945422705483 validation loss:  0.9157196879386902\n",
      "Epoch  20 training loss:  0.9362720987376045 validation loss:  0.8076756596565247\n",
      "Epoch  30 training loss:  0.8980672026381773 validation loss:  0.8028233051300049\n",
      "Epoch  40 training loss:  0.8878775694791008 validation loss:  0.7944649457931519\n",
      "Epoch  50 training loss:  0.8291949173983406 validation loss:  0.8199645280838013\n",
      "Early stopping: Epoch  54 training loss:  0.8167918096570408 validation loss:  0.8184852004051208\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  1  hidden dim:  32\n",
      "Epoch  10 training loss:  0.954272128203336 validation loss:  0.802457869052887\n",
      "Epoch  20 training loss:  0.9276125869330238 validation loss:  0.7893489599227905\n",
      "Epoch  30 training loss:  0.8938044011592865 validation loss:  0.7923874855041504\n",
      "Epoch  40 training loss:  0.8704550283796647 validation loss:  0.7581807374954224\n",
      "Epoch  50 training loss:  0.8358571213834426 validation loss:  0.7427749037742615\n",
      "Epoch  60 training loss:  0.7997289878480575 validation loss:  0.7550206780433655\n",
      "Early stopping: Epoch  68 training loss:  0.7859142519095365 validation loss:  0.7540467381477356\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  2  hidden dim:  32\n",
      "Epoch  10 training loss:  0.9411968325867373 validation loss:  0.8025717735290527\n",
      "Epoch  20 training loss:  0.9067493344054502 validation loss:  0.7856848239898682\n",
      "Epoch  30 training loss:  0.8454731948235455 validation loss:  0.7618339657783508\n",
      "Epoch  40 training loss:  0.8090815316228306 validation loss:  0.7950761318206787\n",
      "Early stopping: Epoch  43 training loss:  0.7953190952539444 validation loss:  0.8025735020637512\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  3  hidden dim:  32\n",
      "Epoch  10 training loss:  0.9589385197443121 validation loss:  0.8398427367210388\n",
      "Epoch  20 training loss:  0.9358489390681771 validation loss:  0.8044050335884094\n",
      "Early stopping: Epoch  24 training loss:  0.9103823952815112 validation loss:  0.8100894689559937\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  4  hidden dim:  32\n",
      "Epoch  10 training loss:  1.0962350123068865 validation loss:  0.9409603476524353\n",
      "Epoch  20 training loss:  1.0100946321206934 validation loss:  0.8494033813476562\n",
      "Epoch  30 training loss:  0.9253441428436953 validation loss:  0.777464747428894\n",
      "Epoch  40 training loss:  0.8875463219249949 validation loss:  0.7924537658691406\n",
      "Early stopping: Epoch  43 training loss:  0.8806647623286528 validation loss:  0.7798936367034912\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  1  hidden dim:  64\n",
      "Epoch  10 training loss:  0.9596571115886464 validation loss:  0.7978902459144592\n",
      "Epoch  20 training loss:  0.9231045526616714 validation loss:  0.7837536931037903\n",
      "Epoch  30 training loss:  0.8758330152315252 validation loss:  0.7650776505470276\n",
      "Early stopping: Epoch  36 training loss:  0.8508948525961708 validation loss:  0.7602769732475281\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  2  hidden dim:  64\n",
      "Epoch  10 training loss:  0.9923770199803745 validation loss:  0.7689809799194336\n",
      "Early stopping: Epoch  20 training loss:  0.9238700410898995 validation loss:  0.7740211486816406\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  3  hidden dim:  64\n",
      "Epoch  10 training loss:  0.9830417597995085 validation loss:  0.8141003251075745\n",
      "Epoch  20 training loss:  0.9503984626601724 validation loss:  0.7836854457855225\n",
      "Epoch  30 training loss:  0.9041954612030703 validation loss:  0.7907182574272156\n",
      "Epoch  40 training loss:  0.8467374198576983 validation loss:  0.7651570439338684\n",
      "Early stopping: Epoch  44 training loss:  0.8220355195157668 validation loss:  0.7660403847694397\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  4  hidden dim:  64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10 training loss:  1.3655987609835232 validation loss:  0.956113338470459\n",
      "Epoch  20 training loss:  1.0056966771097744 validation loss:  0.9004585146903992\n",
      "Epoch  30 training loss:  0.9524121442261864 validation loss:  0.8980969786643982\n",
      "Epoch  40 training loss:  0.9101222034762887 validation loss:  0.7933791279792786\n",
      "Epoch  50 training loss:  0.8785117321154651 validation loss:  0.7834247946739197\n",
      "Epoch  60 training loss:  0.841860467896742 validation loss:  0.8009827733039856\n",
      "Early stopping: Epoch  65 training loss:  0.8469310683362624 validation loss:  0.8027282953262329\n",
      "\n",
      "\n",
      "starting time var type:  0  sequence length:  12  num layers:  1  hidden dim:  128\n",
      "Epoch  10 training loss:  0.9644266314366284 validation loss:  0.8125137090682983\n",
      "Epoch  20 training loss:  0.9325515101937687 validation loss:  0.7886108160018921\n",
      "Epoch  30 training loss:  0.8916696695720449 validation loss:  0.7974967360496521\n"
     ]
    }
   ],
   "source": [
    "# save average validation loss of 10 (patience value) epochs since the best performance, which are the last 10\n",
    "validation_performance = np.zeros((len(dfs), len(seq_lengths), len(hidden_dim_), len(num_layers_)))\n",
    "\n",
    "for df_type, df in enumerate(dfs):\n",
    "    for seq_idx, seq in enumerate(seq_lengths):\n",
    "        \n",
    "        train_x, train_y, val_x, val_y, test_x, test_y = create_torch_dataset(df, seq)\n",
    "                \n",
    "        # create tensor objects\n",
    "        x_train = torch.from_numpy(train_x).float()\n",
    "        y_train = torch.from_numpy(train_y).float()\n",
    "        x_val = torch.from_numpy(val_x).float()\n",
    "        y_val = torch.from_numpy(val_y).float()\n",
    "        x_test = torch.from_numpy(test_x).float()\n",
    "        y_test = torch.from_numpy(test_y).float()\n",
    "\n",
    "        # create training batch\n",
    "        train_data = []\n",
    "        for i in range(len(x_train)):\n",
    "            train_data.append([x_train[i], y_train[i]])\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "        # other parameters for initializing the model\n",
    "        num_train = x_train.shape[0]\n",
    "        input_dim = x_train.shape[2]\n",
    "        \n",
    "        \n",
    "        for hidden_idx, hidden_dim in enumerate(hidden_dim_):\n",
    "            for layer_idx, num_layers in enumerate(num_layers_):\n",
    "                print()\n",
    "                print()\n",
    "                print(\"starting time var type: \", df_type, \" sequence length: \", \n",
    "                      seq, \" num layers: \", num_layers, \" hidden dim: \", hidden_dim)\n",
    "\n",
    "                criterion = QuantileLoss(quantiles)\n",
    "                model = BLSTM(input_dim=input_dim, \n",
    "                              hidden_dim=hidden_dim, \n",
    "                              output_dim=output_dim, \n",
    "                              num_layers=num_layers, \n",
    "                              quantiles=quantiles)\n",
    "                #print(model)\n",
    "                optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "                # Initialize early stopping variables\n",
    "                val_loss_best = np.Inf\n",
    "                patience_cnt = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "                val_losses = []\n",
    "                for t in range(num_epochs): \n",
    "                    \n",
    "                    err = []\n",
    "                    \n",
    "                    # training\n",
    "                    for batch in trainloader:\n",
    "                        inputs, outputs = batch\n",
    "                        model.add_noise_to_weights() # adding noise to lstm weights during the training\n",
    "                        y_train_pred = model(inputs)\n",
    "\n",
    "                        loss = torch.mean(torch.sum(criterion.loss(torch.transpose(y_train_pred,1,2), outputs), dim=2))\n",
    "                        optimiser.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimiser.step()\n",
    "                        err.append(loss.item())\n",
    "\n",
    "                    # validation\n",
    "                    with torch.no_grad():\n",
    "                        preds=model(x_val)\n",
    "                        val_loss = torch.mean(torch.sum(criterion.loss(torch.transpose(preds,1,2), y_val), dim=2)).item()\n",
    "                        val_losses.append(val_loss)\n",
    "\n",
    "\n",
    "                    if val_loss < val_loss_best:\n",
    "                        val_loss_best = val_loss\n",
    "                        patience_cnt = 0\n",
    "                    else:\n",
    "                        patience_cnt +=1\n",
    "                        if patience_cnt == patience:\n",
    "                            print(\"Early stopping: Epoch \", t+1, \"training loss: \", sum(err)/len(err), \"validation loss: \", val_loss)\n",
    "                            break\n",
    "                    \n",
    "                    if (t+1) % 10 == 0:\n",
    "                        print(\"Epoch \", t+1, \"training loss: \", sum(err)/len(err), \"validation loss: \", val_loss)\n",
    "                    \n",
    "                validation_performance[df_type, seq_idx, hidden_idx, layer_idx] = sum(val_losses[-patience:])/patience\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-pollution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-halloween",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sexual-female",
   "metadata": {},
   "source": [
    "## get best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the hyperparameter setting of model with lowest validation loss\n",
    "best = np.amin(validation_performance)\n",
    "result = np.where(validation_performance == best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-warehouse",
   "metadata": {},
   "source": [
    "# Train model with adjusted hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on it after hyperparameters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-growing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "handed-sport",
   "metadata": {},
   "source": [
    "# Test performance and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on the meshed x-axis\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds=model(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = dayahead_scaler.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[dt.datetime(2020, 12, 1, 0, 0):dt.datetime(2020, 12, 31, 23, 0)][['Day-ahead Price [EUR/MWh]']]\n",
    "test_df[test_df.columns] = dayahead_scaler.inverse_transform(test_df.values)\n",
    "for i in range(preds.shape[1]):\n",
    "    y=preds[:,i,:]\n",
    "    test_df[str(i)]=y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = torch.from_numpy(np.transpose(preds,(0,2,1)))\n",
    "y_test_price = torch.from_numpy(test_df['Day-ahead Price [EUR/MWh]'].values.reshape((-1, 24)))\n",
    "\n",
    "# quantile loss in term of dayahead price (not normalize or standardize)\n",
    "torch.mean(torch.sum(criterion.loss(preds_test, y_test_price), dim=2)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-defense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot whole data\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(test_df.index, test_df['Day-ahead Price [EUR/MWh]'].values, 'r', label='expected')\n",
    "\n",
    "plt.fill(np.concatenate([test_df.index, test_df.index[::-1]]),\n",
    "         np.concatenate([test_df['8'].values.flatten(), test_df['0'].values.flatten()]),\n",
    "         alpha=.25, fc='grey', ec='None', label='98% prediction interval')\n",
    "\n",
    "plt.fill(np.concatenate([test_df.index, test_df.index[::-1]]),\n",
    "         np.concatenate([test_df['7'].values.flatten(), test_df['1'].values.flatten()]),\n",
    "         alpha=.5, fc='grey', ec='None', label='90% prediction interval')\n",
    "\n",
    "plt.fill(np.concatenate([test_df.index, test_df.index[::-1]]),\n",
    "         np.concatenate([test_df['6'].values.flatten(), test_df['2'].values.flatten()]),\n",
    "         alpha=.75, fc='grey', ec='None', label='80% prediction interval')\n",
    "\n",
    "plt.fill(np.concatenate([test_df.index, test_df.index[::-1]]),\n",
    "         np.concatenate([test_df['5'].values.flatten(), test_df['3'].values.flatten()]),\n",
    "         alpha=0.9, fc='grey', ec='None', label='50% prediction interval')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Day ahead Price')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 7 days\n",
    "\n",
    "part_df = test_df[dt.datetime(2020, 12, 1, 0, 0):dt.datetime(2020, 12, 8, 0, 0)]\n",
    "part_df\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.plot(part_df.index, part_df['Day-ahead Price [EUR/MWh]'].values, 'r', label='expected')\n",
    "\n",
    "plt.fill(np.concatenate([part_df.index, part_df.index[::-1]]),\n",
    "         np.concatenate([part_df['8'].values.flatten(), part_df['0'].values.flatten()]),\n",
    "         alpha=.25, fc='grey', ec='None', label='98% prediction interval')\n",
    "\n",
    "plt.fill(np.concatenate([part_df.index, part_df.index[::-1]]),\n",
    "         np.concatenate([part_df['7'].values.flatten(), part_df['1'].values.flatten()]),\n",
    "         alpha=.5, fc='grey', ec='None', label='90% prediction interval')\n",
    "\n",
    "plt.fill(np.concatenate([part_df.index, part_df.index[::-1]]),\n",
    "         np.concatenate([part_df['6'].values.flatten(), part_df['2'].values.flatten()]),\n",
    "         alpha=.75, fc='grey', ec='None', label='80% prediction interval')\n",
    "\n",
    "plt.fill(np.concatenate([part_df.index, part_df.index[::-1]]),\n",
    "         np.concatenate([part_df['5'].values.flatten(), part_df['3'].values.flatten()]),\n",
    "         alpha=0.9, fc='grey', ec='None', label='50% prediction interval')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Day ahead Price')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
