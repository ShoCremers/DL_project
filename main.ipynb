{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-building",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dependencies (as taken from assignment 6)\n",
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Additional Setup to use Tensorboard\n",
    "!pip install -q tensorflow\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-shadow",
   "metadata": {},
   "source": [
    "# Load raw data here, normalise values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-harbor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wireless-albania",
   "metadata": {},
   "source": [
    "# Create sliding windows here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can refer here: \n",
    "# https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/\n",
    "# https://curiousily.com/posts/time-series-forecasting-with-lstm-for-daily-coronavirus-cases/\n",
    "#example \n",
    "# def split_data(stock, lookback):\n",
    "#     data_raw = stock.to_numpy() # convert to numpy array\n",
    "#     data = []\n",
    "    \n",
    "#     # create all possible sequences of length seq_len\n",
    "#     for index in range(len(data_raw) - lookback): \n",
    "#         data.append(data_raw[index: index + lookback])\n",
    "    \n",
    "#     data = np.array(data);\n",
    "#     test_set_size = int(np.round(0.2*data.shape[0]));\n",
    "#     train_set_size = data.shape[0] - (test_set_size);\n",
    "    \n",
    "#     x_train = data[:train_set_size,:-1,:]\n",
    "#     y_train = data[:train_set_size,-1,:]\n",
    "    \n",
    "#     x_test = data[train_set_size:,:-1]\n",
    "#     y_test = data[train_set_size:,-1,:]\n",
    "    \n",
    "#     return [x_train, y_train, x_test, y_test]\n",
    "# lookback = 20 # choose sequence length\n",
    "# x_train, y_train, x_test, y_test = split_data(price, lookback)\n",
    "\n",
    "# x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "# x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "# y_train_lstm = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "# y_test_lstm = torch.from_numpy(y_test).type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-intervention",
   "metadata": {},
   "source": [
    "# convert it to pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_size = 100\n",
    "\n",
    "# Convert 1D MNIST data to pytorch tensors\n",
    "tensors_train = torch.Tensor(data['x']), torch.Tensor(data['y']).long()\n",
    "tensors_test = torch.Tensor(data['x_test']),torch.Tensor(data['y_test']).long()\n",
    "\n",
    "# Create training set and test set from tensors\n",
    "train_set = TensorDataset(*tensors_train)\n",
    "test_set = TensorDataset(*tensors_test)\n",
    "\n",
    "# Create dataloaders from the training and test set for easier iteration over the data\n",
    "train_loader = DataLoader(train_set, batch_size=b_size, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=b_size, shufffle=False)\n",
    "\n",
    "# Get some data and check for dimensions\n",
    "input, label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-stadium",
   "metadata": {},
   "source": [
    "# Define LSTM model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1 #if input dataset is like [[x1,x2,x3...],y], no. of features in input, if weather, price and time used, then maybe it should be 3\n",
    "hidden_dim = 10 #no. of neurons in hidden layer\n",
    "num_layers = 2 # no of hidden layers \n",
    "output_dim = 1 #if we predict only 1 value\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_() #hidden layer output\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_() \n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        # Index hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-spotlight",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a writer to write to Tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Some hyperparams\n",
    "epochs = 80\n",
    "\n",
    "\n",
    "lstm = []\n",
    "for t in range(num_epochs):\n",
    "    # Train one epoch\n",
    "    \n",
    "    y_train_pred = model(x_train)\n",
    "    loss = criterion(y_train_pred, y_train_lstm)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    writer.add_scalars(' Loss',\n",
    "                        {'Train': loss },\n",
    "                        epoch)\n",
    "    \n",
    "print('\\nFinished.')\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=7#we use on week worth of data\n",
    "T=24# we use one day of previous data for training\n",
    "D= #len each training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# refer here: https://www.kaggle.com/taronzakaryan/predicting-stock-price-using-lstm-model-pytorch\n",
    "#unnormlaise values: inverse transform\n",
    "# example code\n",
    "# y_test_pred = model(x_test)\n",
    "\n",
    "# # invert predictions\n",
    "# y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
    "# y_train = scaler.inverse_transform(y_train.detach().numpy())\n",
    "# y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
    "# y_test = scaler.inverse_transform(y_test.detach().numpy())\n",
    "\n",
    "# # calculate root mean squared error\n",
    "# trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
    "# print('Train Score: %.2f RMSE' % (trainScore))\n",
    "# testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
    "# print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell once and wait for it to time out\n",
    "# run this cell a second time and you should see the board\n",
    "\n",
    "%tensorboard --logdir runs/ --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
