{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dependencies (as taken from assignment 6)\n",
    "import os\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Additional Setup to use Tensorboard\n",
    "!pip install -q tensorflow\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-excuse",
   "metadata": {},
   "source": [
    "# Load raw data here and convert it to pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-plate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "capital-cloud",
   "metadata": {},
   "source": [
    "# Define LSTM here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_tokens, feature_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Creates a captining model.\n",
    "\n",
    "        Args:\n",
    "            num_tokens: number of diffent tokens which is also known as\n",
    "                vocabulary size\n",
    "            feature_size: image feature dimension extracted from images with a\n",
    "                feature extractor model.\n",
    "            embed_size: vector dimension of word embeddings\n",
    "            hidden_size: LSTM hidden state size\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "\n",
    "        ########################################################################\n",
    "        # TODO: Define embedding, feature projector, lstm and output projector  #\n",
    "        #                               layers.                                #\n",
    "        ########################################################################\n",
    "\n",
    "        # Embedding layer, embeds each token into a vector\n",
    "        self.embedding = nn.Embedding(num_tokens, embed_size)\n",
    "\n",
    "        # Feature projector, projects image features to a hidden sized vector\n",
    "        self.feature_projector = nn.Linear(feature_size, hidden_size)\n",
    "\n",
    "        # LSTM layer, which will try to guess next word\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Output projector, projects hidden states to token scores\n",
    "        self.output_projector = nn.Linear(hidden_size, num_tokens)\n",
    "\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            features: extracted image features with a shape of (N, D)  where N\n",
    "                is batch size and D is number of features.\n",
    "            captions: grand truth image captions with a shape of (N, M) where M\n",
    "                is max caption length. If an image caption is less than max\n",
    "                caption length it should be padded with <NULL> token.\n",
    "\n",
    "        Returns:\n",
    "            scores: token scores with a shape of (N, M, K) where K is number of\n",
    "                tokens (num_tokens).\n",
    "        \"\"\"\n",
    "\n",
    "        scores = None\n",
    "\n",
    "        ########################################################################\n",
    "        #                     TODO: Implement forward pass.                    #\n",
    "        # Steps to follow:                                                     #\n",
    "        #  1. Embed captions                                                   #\n",
    "        #  2. Create hidden state by projecting features with feature projector#\n",
    "        #  3. Create cell state with zeros                                     #\n",
    "        #  4. Caulate output using LSTM                                        #\n",
    "        #  5. Calculate scores by projecting output with output projector      #\n",
    "        # Note: Be careful with dimenstions                                    #\n",
    "        ########################################################################\n",
    "\n",
    "        # Embed captions\n",
    "        embeds = self.embedding(captions)\n",
    "\n",
    "        # Create h0 from image features\n",
    "        h0 = self.feature_projector(features).unsqueeze(0)\n",
    "\n",
    "        # Set cell state to zeros\n",
    "        c0 = torch.zeros_like(h0)\n",
    "\n",
    "        # Calculate LTSM output\n",
    "        output, _ = self.lstm(embeds, (h0, c0))\n",
    "\n",
    "        # Project output to token scores\n",
    "        scores = self.output_projector(output)\n",
    "\n",
    "        ########################################################################\n",
    "        #                         END OF YOUR CODE                             #\n",
    "        ########################################################################\n",
    "\n",
    "        return scores\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, features, start_idx, max_length):\n",
    "        \"\"\"\n",
    "        Generates captions for each given image features.\n",
    "\n",
    "        Args:\n",
    "            features: image features with a shape of (N, D) where N is batch\n",
    "                size and D is number of features.\n",
    "            start_idx: start token (numerical representation of <START>).\n",
    "            max_length: max sampling iteration.\n",
    "        \"\"\"\n",
    "        # Project image features\n",
    "        h0 = self.feature_projector(features).unsqueeze(0)\n",
    "\n",
    "        # Set first token as <START> token\n",
    "        token = torch.tensor([[start_idx]]*len(features), device=features.device)\n",
    "\n",
    "        # Keep tokens, they are our generated captions and each captions\n",
    "        # starts with <START> token\n",
    "        captions = [token]\n",
    "\n",
    "        for i in range(max_length):\n",
    "            # Embed token\n",
    "            embeds = self.embedding(token)\n",
    "\n",
    "            # First iteration, so init hidden state and cell state\n",
    "            # Hidden state will be projected image feature, cell state will\n",
    "            # be zero\n",
    "            if i == 0:\n",
    "                h = h0\n",
    "                c = torch.zeros_like(h0)\n",
    "            \n",
    "            # Get output and next cell and hidden states\n",
    "            output, (h, c) = self.lstm(embeds, (h, c))\n",
    "\n",
    "            # Project output to token scores\n",
    "            scores = self.output_projector(output)\n",
    "\n",
    "            # Get predictions, the predictions will be our next token,\n",
    "            # because our LSTM tries to guess next token if we give it a\n",
    "            # token\n",
    "            token = torch.argmax(scores, dim=2)\n",
    "\n",
    "            # Store the predictions\n",
    "            captions.append(token)\n",
    "        \n",
    "        # Concatenate predicted tokens\n",
    "        captions = torch.cat(captions, dim=1)\n",
    "\n",
    "        return captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-activation",
   "metadata": {},
   "source": [
    "# train and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=7#we use on week worth of data\n",
    "T=24# we use one day of previous data for training\n",
    "D= #len each training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a writer to write to Tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Some hyperparams\n",
    "epochs = 80\n",
    "lr_decay = 1\n",
    "\n",
    "# Number of different words in our vocabulary including special tokens <START>,\n",
    "# <END>, <NULL> and <UNK>\n",
    "#num_tokens = len(coco['vocab']['idx_to_token']) needs editing\n",
    "\n",
    "# Number of image features\n",
    "feature_size = train_features.shape[-1]\n",
    "\n",
    "# Word embedding vector size\n",
    "embed_size = 512\n",
    "\n",
    "# LSTM hidden state dimension\n",
    "hidden_size = 1024\n",
    "\n",
    "model = LSTM(num_tokens, feature_size, embed_size, hidden_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# Create loss function and optimizer and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=NULL_IDX, reduction='sum')\n",
    "optimizer = optim.Adam(model.parameters(), 1e-3)\n",
    "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: lr_decay ** epoch)\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Train one epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    # Iterate through batches\n",
    "    for features, captions in train_loader:\n",
    "        # Move data to target device\n",
    "        features, captions = features.to(device), captions.to(device)\n",
    "\n",
    "        # Define input and target captions\n",
    "        # Input captions will be fed into LSTM and target captions are used for\n",
    "        # loss calculation as LTSM tries to guess next word\n",
    "        input_captions, target_captions = captions[:, :-1], captions[:, 1:]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        output = model(features, input_captions)\n",
    "        loss = criterion(output.transpose(1, 2), target_captions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss\n",
    "    \n",
    "    # Write train loss to Tensorboard\n",
    "    writer.add_scalars('Captioning Loss',\n",
    "                        {'Train': train_loss / len(train_dataset)},\n",
    "                        epoch)\n",
    "    \n",
    "    # Step learning rate scheduler after each epoch\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Validate loss\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Iterate through batches\n",
    "        for features, captions in val_loader:\n",
    "            # Move data to target device\n",
    "            features, captions = features.to(device), captions.to(device)\n",
    "\n",
    "            # Define input and target captions\n",
    "            # Input captions will be fed into LSTM and target captions are used for\n",
    "            # loss calculation as LTSM tries to guess next word\n",
    "            input_captions, target_captions = captions[:, :-1], captions[:, 1:]\n",
    "            output = model(features, input_captions)\n",
    "            loss = criterion(output.transpose(1, 2), target_captions)\n",
    "\n",
    "            val_loss += loss\n",
    "        \n",
    "        # Write loss to Tensorboard\n",
    "        writer.add_scalars('Captioning Loss',\n",
    "                            {'Validation': val_loss / len(val_dataset)},\n",
    "                            epoch)\n",
    "\n",
    "print('\\nFinished.')\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell once and wait for it to time out\n",
    "# run this cell a second time and you should see the board\n",
    "\n",
    "%tensorboard --logdir runs/ --host localhost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
